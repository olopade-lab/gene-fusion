{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parsl\n",
    "from parsl.config import Config\n",
    "from parsl.executors import HighThroughputExecutor\n",
    "\n",
    "config = Config(\n",
    "    executors=[\n",
    "        HighThroughputExecutor(max_workers=40)\n",
    "    ],\n",
    "    strategy=None\n",
    ")\n",
    "parsl.load(config)\n",
    "\n",
    "from src.utils import get_consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarking_dir = '/cephfs/users/annawoodard/gene-fusion/FusionBenchmarking'\n",
    "analysis_tag = 'sim_50'\n",
    "analysis_dir = 'simulated_data/{}'.format(analysis_tag)\n",
    "ctat_dir = '/cephfs/users/annawoodard/gene-fusion/GRCh38_gencode_v31_CTAT_lib_Oct012019.plug-n-play/ctat_genome_lib_build_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = (\n",
    "    \"docker run -v {}:/data trinityctat/fusionbenchmarking \"\n",
    "    \"bash -c \"\n",
    "    \"'find /data/{}/samples -type f | /data/util/make_file_listing_input_table.pl'\"\n",
    ").format(benchmarking_dir, analysis_dir)\n",
    "\n",
    "!$command > $benchmarking_dir/$analysis_dir/simulated_datafusion_result_file_listing.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = (\n",
    "    \"docker run -v {}:/data trinityctat/fusionbenchmarking \"\n",
    "    \"bash -c \"\n",
    "    \"'/usr/local/src/FusionBenchmarking/benchmarking/collect_preds.pl \"\n",
    "    \"/data/{}/simulated_datafusion_result_file_listing.dat'\"\n",
    ").format(benchmarking_dir, analysis_dir)\n",
    "!$command > $benchmarking_dir/$analysis_dir/preds.collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = (\n",
    "    \"docker run -v {}:/data trinityctat/fusionbenchmarking \"\n",
    "    \"bash -c '\"\n",
    "    \"/usr/local/src/FusionBenchmarking/benchmarking/map_gene_symbols_to_gencode.pl \"\n",
    "    \"/data/{}/preds.collected \"\n",
    "    \"/data/resources/genes.coords.gz /data/resources/genes.aliases'\"\n",
    ").format(benchmarking_dir, analysis_dir)\n",
    "\n",
    "!$command > $benchmarking_dir/$analysis_dir/preds.collected.gencode_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -v /cephfs/users/annawoodard/gene-fusion/GRCh38_gencode_v31_CTAT_lib_Oct012019.plug-n-play/ctat_genome_lib_build_dir:/ctat_genome_lib_build_dir -v /cephfs/users/annawoodard/gene-fusion/data/interim/fusions.tsv:/fusions.tsv trinityctat/starfusion:1.8.0 /usr/local/src/STAR-Fusion/FusionInspector/FusionAnnotator/FusionAnnotator --annotate /fusions.tsv --full --genome_lib_dir /ctat_genome_lib_build_dir/ > /cephfs/users/annawoodard/gene-fusion/data/interim/annotated_fusions.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = (\n",
    "    \"docker run \"\n",
    "    \"-v {}:/ctat_genome_lib_build_dir \"\n",
    "    \"-v {}:/data \"\n",
    "    \"trinityctat/fusionbenchmarking \"\n",
    "    \"bash -c '\"\n",
    "    \"FusionAnnotator/FusionAnnotator \"\n",
    "    \"--annotate /data/{}/preds.collected.gencode_mapped -C 2 --genome_lib_dir /ctat_genome_lib_build_dir/'\"\n",
    ").format(ctat_dir, benchmarking_dir, analysis_dir)\n",
    "\n",
    "!$command > $benchmarking_dir/$analysis_dir/preds.collected.gencode_mapped.wAnnot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = (\n",
    "    \"docker run -v {}:/data trinityctat/fusionbenchmarking \"\n",
    "    \"bash -c \"\n",
    "    \"'/usr/local/src/FusionBenchmarking/benchmarking/filter_collected_preds.pl \"\n",
    "    \"/data/{}/preds.collected.gencode_mapped.wAnnot'\"\n",
    ").format(benchmarking_dir, analysis_dir)\n",
    "!$command > $benchmarking_dir/$analysis_dir/preds.collected.gencode_mapped.wAnnot.filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = (\n",
    "    \"docker run -v {}/..:/data trinityctat/fusionbenchmarking \"\n",
    "    \"bash -c \"\n",
    "    \"'chmod -R a+w /data/FusionBenchmarking/{}'\"\n",
    ").format(benchmarking_dir, analysis_dir)\n",
    "!$command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(\n",
    "    os.path.join(benchmarking_dir, analysis_dir, 'preds.collected.gencode_mapped.wAnnot.filt'),\n",
    "    sep='\\t',\n",
    "    skiprows=1,\n",
    "    header=None,\n",
    "    names=['sample', 'prog', 'fusion', 'J', 'S', 'mapped_gencode_A_gene_list', 'mapped_gencode_B_gene_list', 'annotations']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['gene_a'] = [sorted(i.split(','))[0] for i in predictions.mapped_gencode_A_gene_list]\n",
    "predictions['gene_b'] = [sorted(i.split(','))[0] for i in predictions.mapped_gencode_B_gene_list]\n",
    "predictions['normalized_fusion'] = predictions[['gene_a', 'gene_b']].apply(lambda x: '--'.join(sorted(x)), axis=1)\n",
    "predictions['sum_J_S'] = predictions.J + predictions.S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sorted(predictions['sample'].unique())\n",
    "fusion_sets = pd.DataFrame(data={\n",
    "    'prog': [\n",
    "        prog \n",
    "        for prog in sorted(predictions.prog.unique())\n",
    "        for sample in samples\n",
    "    ],\n",
    "    'sample': [\n",
    "        sample\n",
    "        for prog in sorted(predictions.prog.unique())\n",
    "        for sample in samples\n",
    "    ],\n",
    "    'fusion_set': [\n",
    "        set(\n",
    "            predictions[\n",
    "                (predictions['sample'] == sample) & \n",
    "                (predictions['prog'] == prog)\n",
    "            ]['normalized_fusion'].unique()\n",
    "        ) \n",
    "        for prog in sorted(predictions.prog.unique())\n",
    "        for sample in samples\n",
    "    ]\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = pd.read_csv(\n",
    "    os.path.join(benchmarking_dir, analysis_dir, '__analyze_strict/all.AUC.dat'),\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['prog', 'auc']\n",
    ")\n",
    "\n",
    "progs = auc.prog.unique()\n",
    "mean_auc = pd.DataFrame(data={\n",
    "    'prog': progs,\n",
    "    'mean_auc': [auc[auc.prog == prog].mean()[0] for prog in progs]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "top_prog = mean_auc.sort_values(by='mean_auc', ascending=False)['prog'][0]\n",
    "\n",
    "futures = []\n",
    "for num_progs in range(2, 10):\n",
    "    top_progs = mean_auc.sort_values(by='mean_auc', ascending=False)['prog'][:num_progs]\n",
    "    for quorum in range(2, num_progs + 1):\n",
    "        futures += [\n",
    "            (\n",
    "                get_consensus(\n",
    "                    fusion_sets[\n",
    "                        (fusion_sets['sample'] == sample) & \n",
    "                        (fusion_sets['prog'].str.contains('|'.join(top_progs)))\n",
    "                    ].fusion_set.to_list(),\n",
    "                    quorum\n",
    "                ),\n",
    "                sample,\n",
    "                top_progs,\n",
    "                quorum\n",
    "            )\n",
    "            for sample in samples\n",
    "        ]\n",
    "\n",
    "ensemble_predictions = []\n",
    "for f, sample, top_progs, quorum in futures:\n",
    "    ensemble_fusions = list(f.result())\n",
    "    \n",
    "    indices = [\n",
    "        predictions[\n",
    "            (predictions['sample'] == sample) &\n",
    "            (predictions['normalized_fusion'] == ensemble_fusions[i]) &\n",
    "            (predictions['prog'].str.contains('|'.join(top_progs)))\n",
    "        ].sum_J_S.idxmax()\n",
    "        for i in range(len(ensemble_fusions))\n",
    "    ]\n",
    "    ensemble_predictions.append(predictions.loc[indices])\n",
    "    prog = 'quorum_{}(top_{})'.format(quorum, len(top_progs))\n",
    "    ensemble_predictions[-1].prog = prog\n",
    "    print('finished {} ({} fusions for {}) in {:.0f} seconds'.format(prog, len(indices), sample, time.time() - start))\n",
    "    \n",
    "    union_ensemble_fusions = list(set.union(\n",
    "        fusion_sets[\n",
    "            (fusion_sets['sample'] == sample) & \n",
    "            (fusion_sets['prog'] == top_prog)\n",
    "        ].fusion_set.to_list()[0],\n",
    "        ensemble_fusions\n",
    "    ))\n",
    "    \n",
    "    indices = [\n",
    "        predictions[\n",
    "            (predictions['sample'] == sample) &\n",
    "            (predictions['normalized_fusion'] == union_ensemble_fusions[i]) &\n",
    "            (predictions['prog'].str.contains('|'.join(top_progs)))\n",
    "        ].sum_J_S.idxmax()\n",
    "        for i in range(len(union_ensemble_fusions))\n",
    "    ]\n",
    "    ensemble_predictions.append(predictions.loc[indices])\n",
    "    prog = 'union_{}_quorum_{}(top_{})'.format(top_prog, quorum, len(top_progs))\n",
    "    ensemble_predictions[-1].prog = prog\n",
    "    \n",
    "    print('finished {} ({} fusions) in {:.0f} seconds'.format(prog, len(indices), time.time() - start))\n",
    "\n",
    "print('finished in {:.0f} seconds'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.concat([predictions] + ensemble_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv(\n",
    "    os.path.join(benchmarking_dir, analysis_dir, 'preds.collected.gencode_mapped.wAnnot.filt'),\n",
    "    sep='\\t',\n",
    "    index=False,\n",
    "    columns=['sample', 'prog', 'fusion', 'J', 'S', 'mapped_gencode_A_gene_list', 'mapped_gencode_B_gene_list', 'annotations']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = (\n",
    "    \"docker run -v {}/..:/data trinityctat/fusionbenchmarking \"\n",
    "    \"bash -c \"\n",
    "    \"'rm -rf /data/FusionBenchmarking/{}/__*'\"\n",
    ").format(benchmarking_dir, analysis_dir)\n",
    "!$command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = (\n",
    "    \"docker run -v {}/..:/data trinityctat/fusionbenchmarking \"\n",
    "    \"bash -c \"\n",
    "    \"'cd /data/FusionBenchmarking/{}; \"\n",
    "    \"/data/FusionBenchmarking/simulated_data/analyze_simulated_data.pl \"\n",
    "    \"{}.truth_set.dat {}.fusion_TPM_values.dat'\"\n",
    ").format(benchmarking_dir, analysis_dir, analysis_tag, analysis_tag)\n",
    "!$command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = pd.read_csv(\n",
    "    os.path.join(benchmarking_dir, analysis_dir, '__analyze_strict/all.AUC.dat'),\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['prog', 'auc']\n",
    ")\n",
    "\n",
    "progs = auc.prog.unique()\n",
    "mean_auc = pd.DataFrame(data={\n",
    "    'prog': progs,\n",
    "    'mean_auc': [auc[auc.prog == prog].mean()[0] for prog in progs]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"prog\", y=\"auc\", data=auc)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 500\n",
    "mean_auc.sort_values(by='mean_auc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(.8-.76)/.76*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.intersection({1, 2}, {2, 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 [('A',), ('B',), ('C',), ('D',), ('A', 'B'), ('A', 'C'), ('A', 'D'), ('B', 'C'), ('B', 'D'), ('C', 'D'), ('A', 'B', 'C'), ('A', 'B', 'D'), ('A', 'C', 'D'), ('B', 'C', 'D'), ('A', 'B', 'C', 'D')]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "sets = ['A', 'B', 'C', 'D']\n",
    "result = []\n",
    "for i in range(1, len(sets) + 1):\n",
    "    result += list(itertools.combinations(sets, i))\n",
    "\n",
    "print(len(result), result)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combos = []\n",
    "for i in range(1, len(result) + 1):\n",
    "    combos += list(itertools.combinations(result, i))\n",
    "    print(len(combos))\n",
    "print(len(combos), combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
